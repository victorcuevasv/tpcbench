scp to the cluster: 

scp bscdc07:/home/vcuevas/databricksworkspace/tpcdsbench/datavol/logs/power/presto/analytics.log  /home/bscuser/workspacedatabricks/benchinf/analysis/Documents/RESULTS/power/presto/analytics.log 

scp -r hivevol/1GB bscdc07:/home/vcuevas/databricksworkspace/tpcdsbench/hivevol/

scp -r datavol/QueriesPostgres bscdc07:/home/vcuevas/databricksworkspace/tpcdsbench/datavol/

scp -r datavol/QueriesPresto bscdc07:/home/vcuevas/databricksworkspace/tpcdsbench/datavol/



Execute client jar when logged in the container.

#Must add the zookeeper dependency  or it will not be downloaded.   

spark-submit --class org.bsc.dcc.vcv.JavaSparkHiveExample --master spark://sparkhiveservercontainer:7077   --deploy-mode client /temporal/client-1.0-SNAPSHOT.jar            

cp /temporal/zookeeper-3.4.6.jar   /root/.ivy2/jars/org.apache.zookeeper_zookeeper-3.4.6.jar

#Run the Pi example and download the zookeeper dependency at the same time.

spark-submit --packages org.apache.zookeeper:zookeeper:3.4.6 --class org.apache.spark.examples.SparkPi --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.0.jar   

#Run the Spark-Hive example and download the zookeeper dependency at the same time (will because the input file does not exist).

spark-submit --packages org.apache.zookeeper:zookeeper:3.4.6 --class org.apache.spark.examples.sql.hive.JavaSparkHiveExample --deploy-mode client $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.0.jar



Fix problem with failed queries due to memory errors (changes the spark.driver.memory configuration variable).
Add 
--driver-memory 2g
to the spark-submit call


#Copy the results for Spark from the container.
docker cp namenodecontainer:/data/logs/analytics.log .



spark.serializer=org.apache.spark.serializer.JavaSerializer


Should be integer in query 18.

catalog_sales cs_quantity
customer c_birth_year
customer_demographics cd_dep_count

Query 18: ensure removal of the cross join.

SET SESSION join_reordering_strategy=ELIMINATE_CROSS_JOINS;







