FROM ubuntujavahadoop:dev

USER root

ARG UNAME=anyuser
ARG UID=1000
ARG GID=100

RUN groupadd -g $GID -o $UNAME
RUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME

#Install sudo to run the thrift server script as user $UNAME.
RUN apt-get update && apt-get install -y \
    sudo

ARG APACHE_MIRROR=archive.apache.org
ARG POSTGRES_DRIVER_MIRROR=jdbc.postgresql.org
ARG HADOOP_VERSION=2.7.7
ARG HIVE_VERSION=2.3.5
ARG SPARK_VERSION=2.4.3

#Add the generated key to the authorized set.
#The ssh server has been installed in the ubuntujavahadoop image.

COPY ssh /root/.ssh

RUN chmod 600 /root/.ssh/id_rsa

RUN cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    
#The configuration file will enable passwordless login to nodes
#and without confirmation prompt.

COPY conf/ssh_config /root/.ssh/config

#Add the Hadoop installation directory variable and update the path.
#The hadoop files have been downloaded for the ubuntujavahadoop image.

ENV HADOOP_HOME /opt/hadoop-$HADOOP_VERSION
ENV PATH="${PATH}:/opt/hadoop-$HADOOP_VERSION/bin:/opt/hadoop-$HADOOP_VERSION/sbin"

#Copy the Hadoop configuration files.
#The owner of the rest of the hadoop files has been changed in the ubuntujavahadoop image.

COPY etc_hadoop/* /opt/hadoop-$HADOOP_VERSION/etc/hadoop/
COPY conf/slaves /opt/hadoop-$HADOOP_VERSION/etc/hadoop/
RUN chown -R hadoop:hadoop /opt/hadoop-$HADOOP_VERSION/etc/hadoop/

#Format the hdfs namenode.

RUN hdfs namenode -format

#Download and decompress Hive.

RUN wget -q -P /opt http://$APACHE_MIRROR/dist/hive/hive-$HIVE_VERSION/apache-hive-$HIVE_VERSION-bin.tar.gz && \
    tar xzf /opt/apache-hive-$HIVE_VERSION-bin.tar.gz -C /opt && \
    rm -f /opt/apache-hive-$HIVE_VERSION-bin.tar.gz
    
#Add the Hive installation directory variable and update the path.

ENV HIVE_HOME /opt/apache-hive-$HIVE_VERSION-bin
ENV PATH="${PATH}:/opt/apache-hive-$HIVE_VERSION-bin/bin"

#Copy the Hive configuration file which will override selected defaults.

COPY conf/hive-siteExternalMetastore.xml /opt/apache-hive-$HIVE_VERSION-bin/conf/hive-site.xml

#Download the postgres driver.

RUN wget --no-check-certificate -q -P /opt/apache-hive-$HIVE_VERSION-bin/lib \
    https://$POSTGRES_DRIVER_MIRROR/download/postgresql-42.2.5.jar

#Download and decompress Spark.
#Use the version prebuilt for Hadoop, spark-sql does not work in the version without Hadoop.

RUN wget -q -P /opt http://$APACHE_MIRROR/dist/spark/spark-$SPARK_VERSION/spark-$SPARK_VERSION-bin-hadoop2.7.tgz && \
    tar xzf /opt/spark-$SPARK_VERSION-bin-hadoop2.7.tgz -C /opt && \
    rm -f /opt/spark-$SPARK_VERSION-bin-hadoop2.7.tgz
    
#Add the Spark installation directory variable and update the path.

ENV SPARK_HOME /opt/spark-$SPARK_VERSION-bin-hadoop2.7
ENV PATH="${PATH}:/opt/spark-$SPARK_VERSION-bin-hadoop2.7/bin"

#Copy the Spark configuration file which will override selected defaults.

COPY spark_conf/spark-defaults.conf /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/spark-defaults.conf
COPY conf/slaves /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/slaves
COPY conf/hive-siteExternalMetastoreSpark.xml /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/hive-site.xml

#Add the Hadoop jars to the Spark classpath.

ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"   

#Create directory for event log.
RUN mkdir /tmp/spark-events

#Add the environment variable to enable execution with YARN
ENV HADOOP_CONF_DIR /opt/hadoop-$HADOOP_VERSION/etc/hadoop
ENV HIVE_CONF_DIR /opt/spark-$SPARK_VERSION-bin-hadoop2.7/conf/

#Copy postgres driver jar.
RUN cp /opt/apache-hive-$HIVE_VERSION-bin/lib/postgresql-42.2.5.jar /opt/spark-$SPARK_VERSION-bin-hadoop2.7/jars 

#Add hive user and supergroup, also add UNAME to the supergroup.
RUN useradd hive && \
	groupadd supergroup && \
	usermod -a -G supergroup hive && \
	usermod -a -G supergroup $UNAME

#Copy the initialization script for Hadoop, Hive, Spark and make it executable.

COPY startscripts/startSparkMaster.sh /startSparkMaster.sh

RUN chmod +x /startSparkMaster.sh

CMD ["bash"]

ENTRYPOINT [ "bash", "-c", "/startSparkMaster.sh" ]

