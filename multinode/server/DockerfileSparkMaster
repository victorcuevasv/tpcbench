FROM ubuntujavahadoop:dev

USER root

ARG UNAME=anyuser
ARG UID=1000
ARG GID=100

RUN groupadd -g $GID -o $UNAME
RUN useradd -m -u $UID -g $GID -o -s /bin/bash $UNAME

ARG APACHE_MIRROR=apache.rediris.es
ARG POSTGRES_DRIVER_MIRROR=jdbc.postgresql.org

#Add the generated key to the authorized set.
#The ssh server has been installed in the ubuntujavahadoop image.

COPY ssh /root/.ssh

RUN chmod 600 /root/.ssh/id_rsa

RUN cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    
#The configuration file will enable passwordless login to nodes
#and without confirmation prompt.

COPY conf/ssh_config /root/.ssh/config

#Add the Hadoop installation directory variable and update the path.
#The hadoop files have been downloaded for the ubuntujavahadoop image.

ENV HADOOP_HOME /opt/hadoop-2.7.7
ENV PATH="${PATH}:/opt/hadoop-2.7.7/bin:/opt/hadoop-2.7.7/sbin"

#Copy the Hadoop configuration files.
#The owner of the rest of the hadoop files has been changed in the ubuntujavahadoop image.

COPY etc_hadoop/* /opt/hadoop-2.7.7/etc/hadoop/
COPY conf/slaves /opt/hadoop-2.7.7/etc/hadoop/
RUN chown -R hadoop:hadoop /opt/hadoop-2.7.7/etc/hadoop/

#Format the hdfs namenode.

RUN hdfs namenode -format

#Download and decompress Hive.

RUN wget -q -P /opt http://$APACHE_MIRROR/hive/hive-2.3.4/apache-hive-2.3.4-bin.tar.gz && \
    tar xzf /opt/apache-hive-2.3.4-bin.tar.gz -C /opt && \
    rm -f /opt/apache-hive-2.3.4-bin.tar.gz
    
#Add the Hive installation directory variable and update the path.

ENV HIVE_HOME /opt/apache-hive-2.3.4-bin
ENV PATH="${PATH}:/opt/apache-hive-2.3.4-bin/bin"

#Copy the Hive configuration file which will override selected defaults.

COPY conf/hive-siteExternalMetastore.xml /opt/apache-hive-2.3.4-bin/conf/hive-site.xml

#Download the postgres driver.

RUN wget --no-check-certificate -q -P /opt/apache-hive-2.3.4-bin/lib \
    https://$POSTGRES_DRIVER_MIRROR/download/postgresql-42.2.5.jar

#Download and decompress Spark.
#Use the version prebuilt for Hadoop, spark-sql does not work in the version without Hadoop.

RUN wget -q -P /opt http://$APACHE_MIRROR/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz && \
    tar xzf /opt/spark-2.4.0-bin-hadoop2.7.tgz -C /opt && \
    rm -f /opt/spark-2.4.0-bin-hadoop2.7.tgz
    
#Add the Spark installation directory variable and update the path.

ENV SPARK_HOME /opt/spark-2.4.0-bin-hadoop2.7
ENV PATH="${PATH}:/opt/spark-2.4.0-bin-hadoop2.7/bin"

#Copy the Spark configuration file which will override selected defaults.

COPY spark_conf/spark-defaults.conf /opt/spark-2.4.0-bin-hadoop2.7/conf/spark-defaults.conf
COPY conf/slaves /opt/spark-2.4.0-bin-hadoop2.7/conf/slaves
COPY conf/hive-siteExternalMetastoreSpark.xml /opt/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml

#Add the Hadoop jars to the Spark classpath.

ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"   

#Create directory for event log.
RUN mkdir /tmp/spark-events

#Add the environment variable to enable execution with YARN
ENV HADOOP_CONF_DIR /opt/hadoop-2.7.7/etc/hadoop
ENV HIVE_CONF_DIR /opt/spark-2.4.0-bin-hadoop2.7/conf/

#Copy postgres driver jar.
RUN cp /opt/apache-hive-2.3.4-bin/lib/postgresql-42.2.5.jar /opt/spark-2.4.0-bin-hadoop2.7/jars 

#Copy the initialization script for Hadoop, Hive, Spark and make it executable.

COPY startscripts/startSparkMaster.sh /startSparkMaster.sh

RUN chmod +x /startSparkMaster.sh

#Run the Spark-Hive example only to download the jars, it doesn't matter if it fails.

RUN spark-submit \
     --packages org.apache.zookeeper:zookeeper:3.4.6 \
     --class org.apache.spark.examples.sql.hive.JavaSparkHiveExample \
     --deploy-mode client \
     $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.0.jar ; exit 0

#Set permissions for the ivy repository of the root.
RUN chmod -R 777 /root/.ivy2/cache && \
	chmod -R 777 /root/.ivy2/jars

USER $UNAME

#Run the Spark-Hive example only to download the jars, it doesn't matter if it fails.

RUN spark-submit \
     --packages org.apache.zookeeper:zookeeper:3.4.6 \
     --class org.apache.spark.examples.sql.hive.JavaSparkHiveExample \
     --deploy-mode client \
     $SPARK_HOME/examples/jars/spark-examples_2.11-2.4.0.jar ; exit 0

USER root

CMD ["bash"]

ENTRYPOINT [ "bash", "-c", "/startSparkMaster.sh" ]

