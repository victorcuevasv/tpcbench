FROM ubuntujava:dev

USER root

ARG APACHE_MIRROR=apache.rediris.es

#Install netcat to use nc to check connections, ssh and python.

RUN apt-get update && apt-get install -y \
    netcat \
    ssh \
    python2.7 \
    python-pip \
    python-dev

#Add the generated key to the authorized set.

COPY ssh /root/.ssh

RUN chmod 600 /root/.ssh/id_rsa

RUN cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
    
#The configuration file will enable passwordless login to nodes
#and without confirmation prompt.
    
COPY ssh_config /root/.ssh/config

#Download and decompress Hadoop

RUN wget -q -P /opt http://$APACHE_MIRROR/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz && \
    tar xzf /opt/hadoop-2.7.7.tar.gz -C /opt && \
    rm -f /opt/hadoop-2.7.7.tar.gz

#Add the Hadoop installation directory variable and update the path.

ENV HADOOP_HOME /opt/hadoop-2.7.7
ENV PATH="${PATH}:/opt/hadoop-2.7.7/bin:/opt/hadoop-2.7.7/sbin"

#Copy the Hadoop configuration files.

COPY etc_hadoop/* /opt/hadoop-2.7.7/etc/hadoop/

#Use the same slaves file used for Spark for hadoop.

COPY spark_conf/slaves /opt/hadoop-2.7.7/etc/hadoop/

#Change the owner of the Hadoop files.

RUN useradd hadoop && \
    chown -R hadoop:hadoop /opt/hadoop-2.7.7
    
#Download and decompress Spark.
#Use the version prebuilt for Hadoop, spark-sql does not work in the version without Hadoop.

RUN wget -q -P /opt http://$APACHE_MIRROR/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz && \
    tar xzf /opt/spark-2.4.0-bin-hadoop2.7.tgz -C /opt && \
    rm -f /opt/spark-2.4.0-bin-hadoop2.7.tgz
    
#Add the Spark installation directory variable and update the path.

ENV SPARK_HOME /opt/spark-2.4.0-bin-hadoop2.7
ENV PATH="${PATH}:/opt/spark-2.4.0-bin-hadoop2.7/bin"

#Copy the Spark configuration file which will override selected defaults.

COPY spark_conf/spark-defaults.conf /opt/spark-2.4.0-bin-hadoop2.7/conf/spark-defaults.conf
COPY spark_conf/slaves /opt/spark-2.4.0-bin-hadoop2.7/conf/slaves
COPY hive-siteExternalMetastoreSpark.xml /opt/spark-2.4.0-bin-hadoop2.7/conf/hive-site.xml

#Add the Hadoop jars to the Spark classpath.

ENV SPARK_DIST_CLASSPATH="$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"   

#Copy the initialization script for Hadoop, Hive, Spark and make it executable.

COPY startSparkWorker.sh /startSparkWorker.sh

RUN chmod +x /startSparkWorker.sh

CMD ["bash"]

ENTRYPOINT [ "bash", "-c", "/startSparkWorker.sh" ]

