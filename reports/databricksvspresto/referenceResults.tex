\section{Databricks vs. EMR Presto reference results}\label{referenceResults}

This section presents the reference results obtained by executing the full TPC-DS benchmark at the 1 TB scale factor employing the hardware and software configuration described in Section \ref{experimentalSetup}.

\subsection{Data loading}\label{referenceResultsDataLoading}

The TPC-DS Toolkit includes a data generator that produces raw text files that can be subsequently processed by a data framework to generate data files and accompanying metadata suitable for efficient query processing. The metadata is stored in a Hive-compatible metastore and the data in column-storage format files.

The raw text files generated for the 1 TB scale factor chosen for the reference results actually add up to 910.7 GB. The files were generated with 4 parallel streams, thus for some tables the data comprises 4 separate files, which are stored in the same directory. The format is raw text with the octal character \textbackslash 001 used as the column separator, which is the default in both Databricks and EMR Presto. It should be noted, however, that (unlike Databricks) EMR Presto does not allow in its supported syntax to employ an alternative character as separator. All of these files are stored in their respective directories within a common AWS S3 bucket.

The raw text files are used to create external tables by means of custom CREATE TABLE statements for Databricks and EMR Presto. We use these tables only for accessing data, not query processing. Additional CREATE TABLE statements are used to create another set of external tables employing a column-storage format, these meant for query processing. The data from the raw text external tables is fed to the external column-storage tables by means of INSERT INTO statements. The column-storage table files are also stored in S3. The format used for Databricks is Parquet with SNAPPY compression; the files add up to 297.5 GB, yielding a compression ratio of 3.26. Although Presto also supports the Parquet format, experiments showed that better results are achieved with the ORC format, using SNAPPY compression as well. The ORC files for Presto add up to 246.1 GB, resulting in a compression ratio of 3.94.

It is important to remark that for the reference results we present in this section, the tables were not explicitly analyzed to yield table and column statistics for optimization. The generation of table-level or basic statistics is different for each system. EMR Presto generates them by default with the execution of the INSERT INTO statements, which does not occur with Databricks. Thus for the query processing experiments, unless stated otherwise, EMR Presto has basic table statistics available while Databricks does not. In addition, neither system produces by default column-level statistics. Although Presto has the capability to compute column statistics, our EMR Presto configuration employs AWS Glue for table metadata, which does not support column statistics. On the other hand, Databricks does support them and we explore their effects in Section \ref{databricksWithStatistics}. Finally, we remark that we did not consider custom data partitioning in any of our experiments.

The behavior of the systems in regards to the data loading process as described above is summarized in Figure 18.

\begin{figure}
   \begin{center}
   \scalebox{0.65}{\includegraphics[width=7.0in]{imgs/referenceResults/load_totalHrTimeBarChart.pdf}}
   \end{center}
   \caption{Reference results Data Loading Test total time.}
   \label{fig:referenceResultsDataLoading}
\end{figure}

\subsection{Individual query execution (Power Test)}\label{referenceResultsPowerTest}

The data loading process takes about one hour on EMR Presto, whereas on Databricks it takes around 45 minutes. Although the difference is not major, it is significant. We also point out that Presto generates one file for each worker node, thus 8 files are generated in our setup. On the other hand, Databricks generates one file for each task, yielding over 300 files for some tables. Additional experiments would be required to determine if consolidating the files would result in performance improvements.

The TPC-DS schema includes 7 fact tables, one for the product inventory and 2 for each sales channel sales and returns, the channels being stores, catalog, and the web. A total of 17 dimension tables complete the schema. The fact tables have by far the largest data size, which we can confirm by looking at the individual table creation times, shown in Figure 19 and Figure 20. The corresponding table names are given in Table \ref{table:tpcdsTableNames}. We note that the dbgen\_version table was not created since it is not used in queries and it employs data types with no direct equivalent in the SUTs.

\begin{table}
  \centering
	\begin{tabular}{|l|l|l|}
	  \hline
		\makecell[l]{1) call\_center \\
		2) catalog\_page \\
		3) catalog\_returns \\
		4) catalog\_sales \\
		5) customer \\
		6) customer\_address \\
		7) customer\_demographics \\
		8) date\_dim}
		&
		\makecell[l]{
		9) dbgen\_version (skipped) \\
		10) household\_demographics \\
		11) income\_band \\
		12) inventory \\
		13) item \\
		14) promotion \\
		15) reason \\
		16) ship\_mode}
		& 
		\makecell[l]{
		17) store \\
		18) store\_returns \\
		19) store\_sales \\
		20) time\_dim \\
		21) warehouse \\
		22) web\_page \\
		23) web\_returns \\
		24) web\_sales \\
		25) web\_site}
		\\ 
		\hline
	\end{tabular}
	\caption{TPC-DS table names.}
	\label{table:tpcdsTableNames}
\end{table}








