
#Run Spark shell with iceberg enabled

spark-shell --packages "org.apache.iceberg:iceberg-spark3-runtime:0.11.0" --conf "spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions" --conf "spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog" --conf "spark.sql.catalog.spark_catalog.type=hive" --jars "/usr/lib/spark/jars/iceberg-spark3-runtime-0.11.0.jar"  


import org.apache.iceberg.Table
import org.apache.iceberg.catalog.TableIdentifier
import org.apache.iceberg.hive.HiveCatalog
import org.apache.iceberg.Actions

val tableId = TableIdentifier.of("tpcds_sparkemr_620_1gb_1_1617821720icebtest", "store_sales_denorm_iceberg")
val table = catalog.loadTable(tableId);
val catalog = new HiveCatalog(spark.sparkContext.hadoopConfiguration)
//Rewrite with a size of 100 mb
Actions.forTable(table).rewriteDataFiles().targetSizeInBytes(100 * 1024 * 1024).execute()

//Expire commits older than a given timestamp (note that the supplied value is a long)
table.expireSnapshots().expireOlderThan(1617829183372L).commit()

//Call a maintenance function (spark_catalog is set as a Spark conf)
spark.sql("CALL spark_catalog.system.rollback_to_snapshot('store_sales_denorm_iceberg', 9168233251967364780)").collect().foreach(println)

