
#Obtain the fingerprint for the keypair that will be generated in AWS

openssl pkey -in id_rsa -pubout -outform DER | openssl md5 -c

#Extract the public key (that needs to be submitted to AWS) from the private key file.

openssl rsa -in id_rsa -pubout -out extracted.pub -outform DER

#Convert the file generated in the step above to base 64

openssl base64 -in extracted.pub -out extracted64.pub



#SSH login to the master node.

ssh -i id_rsa hadoop@ec2-54-218-139-60.us-west-2.compute.amazonaws.com

#SSH login to an ec2 instance.

ssh -i id_rsa ec2-user@ec2-54-218-139-60.us-west-2.compute.amazonaws.com


#Run docker without sudo in the master node (logged in as ec2-user).

sudo groupadd docker         (group already created)

sudo usermod -aG docker $USER

Logout and then login to see group membership change.



#Spark configuration directory.

/usr/lib/spark/conf

#Run SparkPi example.

sudo spark-submit --deploy-mode client --class org.apache.spark.examples.SparkPi  /usr/lib/spark/examples/jars/spark-examples_2.11-2.4.0.jar            

#Run SparkPi example.

sudo spark-submit --deploy-mode client --class org.apache.spark.examples.sql.hive.JavaSparkHiveExample  /usr/lib/spark/examples/jars/spark-examples_2.11-2.4.0.jar      


#Get the full name of the master node.

echo $(hostname -f)

ip-172-31-25-127.us-west-2.compute.internal


#Update the metastore for a new namenode.

#Obtain the rootFS for the current namenode.

hive --service metatool -listFSRoot

#Update from the old value to the new value.

#NOTE: does not seem to work, do the change in the Glue GUI at the AWS portal instead.

hive --service metatool -dryRun -updateLocation <new_value> <old_value>

example:

hive --service metatool -dryRun -updateLocation \
hdfs://ip-172-31-21-222.us-west-2.compute.internal:8020/user/hive/warehouse \
hdfs://ip-172-31-30-11.us-west-2.compute.internal:8020/user/hive/warehouse



#Enable presto to drop tables.

#Add the following line to /etc/presto/conf/catalog/hive.properties

hive.allow-drop-table=true

#then restart the presto server

sudo restart presto-server



#Run presto cli

presto-cli --catalog hive --schema default

#Show table schemas.

presto:default> describe hive.default.inventory;

#Run beeline

beeline> !connect jdbc:hive2://localhost:10000/default

#Show table metadata.

beeline> describe extended inventory;

#Delete the directories in hdfs associated with the tables.

hadoop fs -rm -r /user/hive/warehouse/*











UNUSED

#SSH login problem.

ssh -i id_rsa -o "ProxyCommand nc %h %p" hadoop@ec2-34-208-100-49.us-west-2.compute.amazonaws.com

Presto massage create tables

Full thread dump Java HotSpot(TM) 64-Bit Server VM




#
# java.lang.OutOfMemoryError: Java heap space
# -XX:OnOutOfMemoryError="kill -9 %p"
#   Executing /bin/sh -c "kill -9 11686"...
emr/runclient_executequeriesspark.sh: line 23: 11686 Killed                  
spark-submit --conf spark.eventLog.enabled=true 
--packages org.apache.logging.log4j:log4j-api:2.8.2,org.apache.logging.log4j:log4j-core:2.8.2,org.apache.zookeeper:zookeeper:3.4.6 
--conf spark.local.dir=/home/$USER_NAME/tmp 
--conf spark.eventLog.dir=/home/$USER_NAME/tmp 
--class org.bsc.dcc.vcv.ExecuteQueriesSpark 
--master yarn 
--deploy-mode client 
$DIR/../client/project/targetspark/client-1.0-SNAPSHOT.jar /data results plans $DIR/../client/project/targetspark/client-1.0-SNAPSHOT.jar spark true true $1       








